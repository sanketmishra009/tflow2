{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , models\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "class selfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self , dimentions):\n",
    "        super(selfAttention , self).__init__()\n",
    "        self.d = dimentions\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.Wq = self.add_weight(shape = (input_shape[-1] , self.d) , initializer = 'glorot_uniform' , trainable = True , dtype = 'float32')\n",
    "        self.Wk = self.add_weight(shape = (input_shape[-1] , self.d) , initializer = 'glorot_uniform' , trainable = True , dtype = 'float32')\n",
    "        self.Wv = self.add_weight(shape = (input_shape[-1] , self.d) , initializer = 'glorot_uniform' , trainable = True , dtype = 'float32')\n",
    "    \n",
    "    def call(self, q_x, k_x, v_x, mask=None):\n",
    "        # Computing query, key and value\n",
    "        q = tf.matmul(q_x,self.Wq) #[None, t, d]\n",
    "        k = tf.matmul(k_x,self.Wk) #[None, t, d]\n",
    "        v = tf.matmul(v_x,self.Wv) #[None, t, d]\n",
    "        \n",
    "        # Computing the probability matrix\n",
    "        p = tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d) # [None, t, t]\n",
    "                \n",
    "        if mask is None:\n",
    "            p = tf.nn.softmax(p)\n",
    "        else:\n",
    "            # Creating the mask\n",
    "            p += mask * -1e9\n",
    "            p = tf.nn.softmax(p)\n",
    "                \n",
    "        # Computing the final output\n",
    "        h = tf.matmul(p, v) # [None, t, t] . [None, t, d] => [None, t, d]\n",
    "        return h,p\n",
    "n_seq = 7\n",
    "x = tf.constant(np.random.normal(size=(1,n_seq,512)), dtype='float32')\n",
    "layer = selfAttention(512)\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n",
    "h, p = layer(x, x, x, mask)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fclayer(tf.keras.layers.Layer):\n",
    "    def __init__(self , d1 , d2):\n",
    "        super(fclayer , self).__init__()\n",
    "        self.layer1 = tf.keras.layers.Dense(d1 , activation = 'relu')\n",
    "        self.layer2 = tf.keras.layers.Dense(d2)\n",
    "    \n",
    "    def call(self , x):\n",
    "        x = self.layer1(x)\n",
    "        y = self.layer2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self , d , n_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Feature dimensionality\n",
    "        self.d = d\n",
    "        # Dimensionality of a head\n",
    "        self.d_head = int(d/n_heads) \n",
    "        # Number of heads\n",
    "        self.n_heads = n_heads\n",
    "        # Actual attention heads\n",
    "        self.attn_heads = [selfAttention(self.d_head) for i in range(self.n_heads)]\n",
    "        # Fully connected layer\n",
    "        self.fclayer = fclayer(2048, self.d)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        def compute_multihead_output(x):\n",
    "            \"\"\" Computing the multi head attention output\"\"\"\n",
    "            outputs = [head(x, x, x)[0] for head in self.attn_heads]            \n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "            return outputs\n",
    "        \n",
    "        h1 = compute_multihead_output(x)\n",
    "        y = self.fclayer(h1)\n",
    "        \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \"\"\" The decoder layer \"\"\"\n",
    "    def __init__(self, d, n_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Feature dimensionality\n",
    "        self.d = d\n",
    "        # Dimensionality of a single head\n",
    "        self.d_head = int(d/n_heads)\n",
    "        # Actual self attention heads (decoder inputs)\n",
    "        self.dec_attn_heads = [selfAttention(self.d_head) for i in range(n_heads)]\n",
    "        # Actual self attention heads (encoder outputs)\n",
    "        self.attn_heads = [selfAttention(self.d_head) for i in range(n_heads)]\n",
    "        # Fully connected layer\n",
    "        self.fc_layer = fclayer(2048, self.d)\n",
    "        \n",
    "    def call(self, de_x, en_x, mask=None):\n",
    "        \n",
    "        def compute_multihead_output(attn_heads, de_x, en_x, mask=None):\n",
    "            \"\"\" Computing the multi head attention output\"\"\"\n",
    "            outputs = [head(en_x, en_x, de_x, mask)[0] for head in attn_heads]\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "            return outputs\n",
    "        \n",
    "        # Multi head attention layer output (from decoder inputs)\n",
    "        h1 = compute_multihead_output(self.dec_attn_heads, de_x, de_x, mask)        \n",
    "        # Multi head attention layer output (from encoder outputs)\n",
    "        h2 = compute_multihead_output(self.attn_heads, h1, en_x)\n",
    "        y = self.fc_layer(h2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 25\n",
    "env = 300\n",
    "dv = 400\n",
    "n_head = 8\n",
    "d = 512\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((n_steps , n_steps)), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanketmishra/Desktop/Desktop/college/PhD/.conda/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "en_inp = layers.Input(shape = (n_steps,))\n",
    "en_emb = layers.Embedding(env , d, input_length = n_steps)(en_inp)\n",
    "eno1 = EncoderLayer(d , n_head)(en_emb)\n",
    "en02 = EncoderLayer(d , n_head)(eno1)\n",
    "\n",
    "de_inp = layers.Input(shape=(n_steps,))\n",
    "de_emb = layers.Embedding(dv, 512, input_length=n_steps)(de_inp)\n",
    "de_out1 = DecoderLayer(d, n_head)(de_emb, en02, mask)\n",
    "de_out2 = DecoderLayer(d, n_head)(de_out1, en02, mask)\n",
    "de_pred = layers.Dense(dv, activation='softmax')(de_out2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MinTransformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"MinTransformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_19      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_19        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">153,600</span> │ input_layer_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_20      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_layer_24    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,886,144</span> │ embedding_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderLayer</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_20        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">204,800</span> │ input_layer_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_layer_25    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,886,144</span> │ encoder_layer_24… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderLayer</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_layer_7     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,672,576</span> │ embedding_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderLayer</span>)      │                   │            │ encoder_layer_25… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_layer_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,672,576</span> │ decoder_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderLayer</span>)      │                   │            │ encoder_layer_25… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">205,200</span> │ decoder_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_19      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_19        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m153,600\u001b[0m │ input_layer_19[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_20      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_layer_24    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m2,886,144\u001b[0m │ embedding_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEncoderLayer\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_20        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m204,800\u001b[0m │ input_layer_20[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_layer_25    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m2,886,144\u001b[0m │ encoder_layer_24… │\n",
       "│ (\u001b[38;5;33mEncoderLayer\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_layer_7     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m3,672,576\u001b[0m │ embedding_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDecoderLayer\u001b[0m)      │                   │            │ encoder_layer_25… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_layer_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m3,672,576\u001b[0m │ decoder_layer_7[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDecoderLayer\u001b[0m)      │                   │            │ encoder_layer_25… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_61 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m400\u001b[0m)   │    \u001b[38;5;34m205,200\u001b[0m │ decoder_layer_8[\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,681,040</span> (52.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,681,040\u001b[0m (52.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,681,040</span> (52.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,681,040\u001b[0m (52.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer = models.Model(\n",
    "    inputs=[en_inp, de_inp], outputs=de_pred, name='MinTransformer'\n",
    ")\n",
    "transformer.compile(\n",
    "    loss='categorical_crossentropy', optimizer='adam', metrics=['acc']\n",
    ")\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough Cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 7, 512])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nseq = 7\n",
    "x = tf.constant(np.random.normal(size = (1 , nseq , 512)))\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((7,7)) , -1 , 0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         1.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]\n",
      " [0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715\n",
      "  0.14285715]]\n"
     ]
    }
   ],
   "source": [
    "layer = selfAttention(512)\n",
    "p , h = layer(x, x, x, mask)\n",
    "print(p.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "ff = fclayer(2048, 512)(h)\n",
    "print(ff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
