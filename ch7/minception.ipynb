{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "# import requests\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "import zipfile\n",
    "# import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, AvgPool2D, Dense, Concatenate, Flatten, Lambda, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data','tiny-imagenet-200.zip')):\n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data','tiny-imagenet-200.zip'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "else:\n",
    "    print(\"The zip file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'tiny-imagenet-200')):\n",
    "    with zipfile.ZipFile(os.path.join('data','tiny-imagenet-200.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "else:\n",
    "    print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_test_labels_df(test_labels_path):\n",
    "    \"\"\" Reading the test data labels for all files in the test set as a data frame \"\"\"\n",
    "    test_df = pd.read_csv(test_labels_path, sep='\\t', index_col=None, header=None)\n",
    "    test_df = test_df.iloc[:,[0,1]].rename({0:\"filename\", 1:\"class\"}, axis=1)\n",
    "    return test_df\n",
    "        \n",
    "\n",
    "def get_train_valid_test_data_generators(batch_size, target_size):\n",
    "    \"\"\" Get the training/validation/testing data generators \"\"\"\n",
    "    \n",
    "    # Code listing 7.1\n",
    "    # Defining a data-augmenting image data generator and a standard image data generator\n",
    "    image_gen_aug = ImageDataGenerator(\n",
    "        samplewise_center=False, rotation_range=30, width_shift_range=0.2,\n",
    "        height_shift_range=0.2, brightness_range=(0.5,1.5), shear_range=5, \n",
    "        zoom_range=0.2, horizontal_flip=True, fill_mode='reflect', validation_split=0.1\n",
    "    )\n",
    "    image_gen = ImageDataGenerator(samplewise_center=False)\n",
    "    \n",
    "    # Code listing 7.2\n",
    "    # Define a training data generator\n",
    "    partial_flow_func = partial(\n",
    "        image_gen_aug.flow_from_directory, \n",
    "        directory=os.path.join('data','tiny-imagenet-200', 'train'), \n",
    "        target_size=target_size, classes=None,\n",
    "        class_mode='categorical', batch_size=batch_size, \n",
    "        shuffle=True, seed=random_seed)\n",
    "    \n",
    "    # Get the training data subset\n",
    "    train_gen = partial_flow_func(subset='training')\n",
    "    # Get the validation data subset\n",
    "    valid_gen = partial_flow_func(subset='validation')    \n",
    "\n",
    "    # Defining the test data generator\n",
    "    test_df = get_test_labels_df(os.path.join('data','tiny-imagenet-200',  'val', 'val_annotations.txt'))\n",
    "    test_gen = image_gen.flow_from_dataframe(\n",
    "        test_df, directory=os.path.join('data','tiny-imagenet-200',  'val', 'images'), target_size=target_size, classes=None,\n",
    "        class_mode='categorical', batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    return train_gen, valid_gen, test_gen\n",
    "\n",
    "# Code listing 7.3\n",
    "def data_gen_augmented_inceptionnet_v1(gen, random_gamma=False, random_occlude=False):\n",
    "    for x,y in gen: \n",
    "        \n",
    "        if x.ndim != 4:\n",
    "            raise ValueError(\"This function is designed for a batch of images with 4 dims [b, h, w, c]\")\n",
    "            \n",
    "        if random_gamma:\n",
    "            # Gamma correction\n",
    "            # Doing this in the image process fn doesn't help improve performance\n",
    "            rand_gamma = np.random.uniform(0.9, 1.08, (x.shape[0],1,1,1))\n",
    "            x = x**rand_gamma\n",
    "        \n",
    "        if random_occlude:\n",
    "            # Randomly occluding sections in the image\n",
    "            occ_size = 10\n",
    "            occ_h, occ_w = np.random.randint(0, x.shape[1]-occ_size), np.random.randint(0, x.shape[2]-occ_size)\n",
    "            x[::2,occ_h:occ_h+occ_size,occ_w:occ_w+occ_size,:] = np.random.choice([0.,128.,255.])\n",
    "        \n",
    "        # Image centering\n",
    "        x -= np.mean(x, axis=(1,2,3), keepdims=True)\n",
    "        \n",
    "        # Making sure we replicate the target (y) three times\n",
    "        yield x,(y,y,y)\n",
    "        \n",
    "batch_size = 128\n",
    "target_size = (56,56)\n",
    "# Getting the train,valid, test data generators\n",
    "train_gen, valid_gen, test_gen = get_train_valid_test_data_generators(batch_size, target_size)\n",
    "# Modifying the data generators to fit the model targets\n",
    "# We augment data in the training set\n",
    "train_gen_aux = data_gen_augmented_inceptionnet_v1(train_gen, random_gamma=True, random_occlude=True)\n",
    "# We do not augment data in the validation/test datasets\n",
    "valid_gen_aux = data_gen_augmented_inceptionnet_v1(valid_gen)\n",
    "test_gen_aux = data_gen_augmented_inceptionnet_v1(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "all_labels = []\n",
    "n_trials = 10\n",
    "\n",
    "valid_gen_test = tee(valid_gen, n_trials)\n",
    "\n",
    "for i in range(n_trials):    \n",
    "    labels = []\n",
    "    for j in range(5):\n",
    "        _, ohe = next(valid_gen_test[i])\n",
    "        # Convert one hot encoded to class labels\n",
    "        labels.append(np.argmax(ohe, axis=-1))\n",
    "        \n",
    "    # Concat all labels\n",
    "    labels = np.reshape(np.concatenate(labels, axis=0), (1,-1))        \n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Concat all labels accross all trials\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Assert the labels are equal across all trials\n",
    "assert np.all(np.all(all_labels == all_labels[0,:], axis=0)), \"Labels across multiple trials were not equal\"\n",
    "print(\"Successful! Labels across all trials were consistent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = [] # Holds both training and validation samples to be plotted\n",
    "\n",
    "# Getting training samples (20 samples)\n",
    "for i, (x,y) in enumerate(train_gen_aux):\n",
    "    if i>=20: break\n",
    "    data.append((x[0,:,:,:]+128).astype('int32'))\n",
    "    \n",
    "# Getting validation samples (20 samples)\n",
    "for i,(x,y) in enumerate(valid_gen_aux):\n",
    "    if i>=20: break\n",
    "    data.append((x[0,:,:,:]+128).astype('int32'))\n",
    "    \n",
    "# Creating a plot with 40 subplots (4 rows and 10 columns)\n",
    "n_rows = 4\n",
    "n_cols = 10\n",
    "f, axes = plt.subplots(n_rows, n_cols, figsize=(18,9))\n",
    "\n",
    "# Plot the training and validation images\n",
    "# First 2 rows are training data\n",
    "# Second 2 rows are validation data\n",
    "for ri in range(n_rows):\n",
    "    for ci in range(n_cols):\n",
    "        # Plotting the correct image at ri,ci position in the plot\n",
    "        i = ri*n_cols + ci\n",
    "        axes[ri][ci].imshow(data[i])\n",
    "        axes[ri][ci].axis('off')\n",
    "        \n",
    "        # Setting plot titles\n",
    "        if ri==0 and ci == n_cols//2:\n",
    "            axes[ri][ci].set_title(\"Training data\", fontsize=20, pad=1, x=-0.15)\n",
    "        elif ri==2 and ci == n_cols/2:\n",
    "            axes[ri][ci].set_title(\"Validation data\", fontsize=20, pad=1, x=-0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen_augmented_minception(gen, random_gamma=False, random_occlude=False):\n",
    "    for x,y in gen: \n",
    "        if x.ndim != 4:\n",
    "            raise ValueError(\"This function is designed for a batch of images with 4 dims [b, h, w, c]\")\n",
    "            \n",
    "        if random_gamma:\n",
    "            # Gamma correction\n",
    "            # Doing this in the image process fn doesn't help improve performance\n",
    "            rand_gamma = np.random.uniform(0.93, 1.06, (x.shape[0],1,1,1))\n",
    "            x = x**rand_gamma\n",
    "        \n",
    "        if random_occlude:\n",
    "            # Randomly occluding sections in the image\n",
    "            occ_size = 10\n",
    "            occ_h, occ_w = np.random.randint(0, x.shape[1]-occ_size), np.random.randint(0, x.shape[2]-occ_size)\n",
    "            x[::2,occ_h:occ_h+occ_size,occ_w:occ_w+occ_size,:] = np.random.choice([0.,128.,255.])\n",
    "        \n",
    "        # Making sure we replicate the target (y) three times\n",
    "        yield x,y\n",
    "        \n",
    "        \n",
    "batch_size = 128\n",
    "target_size = (64,64)\n",
    "# Getting the train,valid, test data generators\n",
    "train_gen, valid_gen, test_gen = get_train_valid_test_data_generators(batch_size, target_size)\n",
    "# Modifying the data generators to fit the model targets\n",
    "# We augment data in the training set\n",
    "train_gen_aux = data_gen_augmented_minception(train_gen, random_gamma=True, random_occlude=True)\n",
    "# We do not augment data in the validation/test datasets\n",
    "valid_gen_aux = data_gen_augmented_minception(valid_gen)\n",
    "test_gen_aux = data_gen_augmented_minception(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dropout, AvgPool2D, Dense, Concatenate, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import RandomCrop, RandomContrast\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "\n",
    "K.clear_session()\n",
    "def get_minception_resnet_v2():\n",
    "\n",
    "    def stem(inp, activation='relu', bn=True):\n",
    "    \n",
    "        conv1_1 = Conv2D(32, (3,3), strides=(2,2), activation=None, kernel_initializer=init, padding='same')(inp) #62x62\n",
    "        if bn:\n",
    "            conv1_1 = BatchNormalization()(conv1_1)\n",
    "        conv1_1 = Activation(activation)(conv1_1)\n",
    "        \n",
    "        conv1_2 = Conv2D(32, (3,3), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(conv1_1) # 31x31\n",
    "        if bn:\n",
    "            conv1_2 = BatchNormalization()(conv1_2)\n",
    "        conv1_2 = Activation(activation)(conv1_2)\n",
    "        \n",
    "        conv1_3 = Conv2D(64, (3,3), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(conv1_2) # 31x31\n",
    "        if bn:\n",
    "            conv1_3 = BatchNormalization()(conv1_3)\n",
    "        conv1_3 = Activation(activation)(conv1_3)\n",
    "        \n",
    "        # Split to two branches\n",
    "        # Branch 1\n",
    "        maxpool2_1 = MaxPool2D((3,3), strides=(2,2), padding='same')(conv1_3)\n",
    "        \n",
    "        # Branch 2\n",
    "        conv2_2 = Conv2D(96, (3,3), strides=(2,2), activation=None, kernel_initializer=init, padding='same')(conv1_3)\n",
    "        if bn:\n",
    "            conv2_2 = BatchNormalization()(conv2_2)\n",
    "        conv2_2 = Activation(activation)(conv2_2)\n",
    "        \n",
    "        # Concat the results from two branches\n",
    "        out2 = Concatenate(axis=-1)([maxpool2_1, conv2_2])\n",
    "        \n",
    "        # Split to two branches\n",
    "        # Branch 1\n",
    "        conv3_1 = Conv2D(64, (1,1), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(out2)\n",
    "        if bn:\n",
    "            conv3_1 = BatchNormalization()(conv3_1)\n",
    "        conv3_1 = Activation(activation)(conv3_1)\n",
    "        \n",
    "        conv3_2 = Conv2D(96, (3,3), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(conv3_1)\n",
    "        if bn:\n",
    "            conv3_2 = BatchNormalization()(conv3_2)\n",
    "        conv3_2 = Activation(activation)(conv3_2)\n",
    "        \n",
    "        # Branch 2\n",
    "        conv4_1 = Conv2D(64, (1,1), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(out2)\n",
    "        if bn:\n",
    "            conv4_1 = BatchNormalization()(conv4_1)\n",
    "        conv4_1 = Activation(activation)(conv4_1)\n",
    "        \n",
    "        conv4_2 = Conv2D(64, (7,1), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(conv4_1)\n",
    "        if bn:\n",
    "            conv4_2 = BatchNormalization()(conv4_2)\n",
    "        \n",
    "        conv4_3 = Conv2D(64, (1,7), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(conv4_2)\n",
    "        if bn:\n",
    "            conv4_3 = BatchNormalization()(conv4_3)\n",
    "        conv4_3 = Activation(activation)(conv4_3)\n",
    "        \n",
    "        conv4_4 = Conv2D(96, (3,3), strides=(1,1), activation=None, kernel_initializer=init, padding='same')(conv4_3)\n",
    "        if bn:\n",
    "            conv4_4 = BatchNormalization()(conv4_4)\n",
    "        conv4_4 = Activation(activation)(conv4_4)\n",
    "        \n",
    "        # Concat the results from two branches\n",
    "        out34 = Concatenate(axis=-1)([conv3_2, conv4_4])\n",
    "        \n",
    "        # Split to two branches\n",
    "        # Branch 1\n",
    "        maxpool5_1 = MaxPool2D((3,3), strides=(2,2), padding='same')(out34)\n",
    "        # Branch 2\n",
    "        conv6_1 = Conv2D(192, (3,3), strides=(2,2), activation=None, kernel_initializer=init, padding='same')(out34)\n",
    "        if bn:\n",
    "            conv6_1 = BatchNormalization()(conv6_1)\n",
    "        conv6_1 = Activation(activation)(conv6_1)\n",
    "        \n",
    "        # Concat the results from two branches\n",
    "        out56 = Concatenate(axis=-1)([maxpool5_1, conv6_1])\n",
    "        \n",
    "        return out56\n",
    "    def inception_resnet_a(inp, n_filters, initializer, activation='relu', bn=True, res_w=0.1):\n",
    "        \n",
    "        # Split to three branches\n",
    "        # Branch 1\n",
    "        out1_1 = Conv2D(n_filters[0][0], (1,1), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(inp)\n",
    "        if bn:\n",
    "            out1_1 = BatchNormalization()(out1_1)\n",
    "        out1_1 = Activation(activation)(out1_1)\n",
    "            \n",
    "        # Branch 2\n",
    "        out2_1 = Conv2D(n_filters[1][0], (1,1), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(inp)\n",
    "        if bn:\n",
    "            out2_1 = BatchNormalization()(out2_1)\n",
    "        out2_1 = Activation(activation)(out2_1)\n",
    "        \n",
    "        out2_2 = Conv2D(n_filters[1][1], (1,1), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(out2_1)\n",
    "        if bn:\n",
    "            out2_2 = BatchNormalization()(out2_2)\n",
    "        out2_2 = Activation(activation)(out2_2)\n",
    "        \n",
    "        # Branch 3\n",
    "        out3_1 = Conv2D(n_filters[2][0], (1,1), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(inp)\n",
    "        if bn:\n",
    "            out3_1 = BatchNormalization()(out3_1)\n",
    "        out3_1 = Activation(activation)(out3_1)\n",
    "        \n",
    "        out3_2 = Conv2D(n_filters[2][1], (3,3), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(out3_1)\n",
    "        if bn:\n",
    "            out3_2 = BatchNormalization()(out3_2)\n",
    "        out3_2 = Activation(activation)(out3_2)\n",
    "        \n",
    "        out3_3 = Conv2D(n_filters[2][2], (3,3), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(out3_2)\n",
    "        if bn:\n",
    "            out3_3 = BatchNormalization()(out3_3)\n",
    "        out3_3 = Activation(activation)(out3_3)\n",
    "        \n",
    "        out3_4 = Conv2D(n_filters[2][3], (1,1), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(out3_3)\n",
    "        if bn:\n",
    "            out3_4 = BatchNormalization()(out3_4)\n",
    "        out3_4 = Activation(activation)(out3_4)\n",
    "        \n",
    "        # Concat the results from three branches\n",
    "        out4_1 = Concatenate(axis=-1)([out1_1, out2_2, out3_4])\n",
    "        out4_2 = Conv2D(n_filters[3][0], (1,1), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(out4_1)\n",
    "        if bn:\n",
    "            out4_2 = BatchNormalization()(out4_2)\n",
    "        \n",
    "        # Residual connection\n",
    "        out4_2 += res_w * inp\n",
    "        \n",
    "        # Last activation\n",
    "        out4_2 = Activation(activation)(out4_2)\n",
    "        \n",
    "        return out4_2\n",
    "    \n",
    "    def inception_resnet_b(inp, n_filters, initializer, activation='relu', bn=True, res_w=0.1):\n",
    "        \n",
    "        # Split to two branches\n",
    "        # Branch 1\n",
    "        out1_1 = Conv2D(n_filters[0][0], (1,1), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(inp)\n",
    "        if bn:\n",
    "            out1_1 = BatchNormalization()(out1_1)\n",
    "        out1_1 = Activation(activation)(out1_1)\n",
    "        \n",
    "        # Branch 2\n",
    "        out2_1 = Conv2D(n_filters[1][0], (1,1), strides=(1,1), activation=activation, \n",
    "                        kernel_initializer=initializer, padding='same')(inp)\n",
    "        if bn:\n",
    "            out2_1 = BatchNormalization()(out2_1)\n",
    "        out2_1 = Activation(activation)(out2_1)\n",
    "        \n",
    "        out2_2 = Conv2D(n_filters[1][1], (1,7), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(out2_1)\n",
    "        if bn:\n",
    "            out2_2 = BatchNormalization()(out2_2)\n",
    "        out2_2 = Activation(activation)(out2_2)\n",
    "        \n",
    "        out2_3 = Conv2D(n_filters[1][2], (7,1), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(out2_2)\n",
    "        if bn:\n",
    "            out2_3 = BatchNormalization()(out2_3)\n",
    "        out2_3 = Activation(activation)(out2_3)\n",
    "        \n",
    "        # Concat the results from 2 branches\n",
    "        out3_1 = Concatenate(axis=-1)([out1_1, out2_3])\n",
    "        out3_2 = Conv2D(n_filters[2][0], (1,1), strides=(1,1), activation=None, \n",
    "                        kernel_initializer=initializer, padding='same')(out3_1)\n",
    "        if bn:\n",
    "            out3_2 = BatchNormalization()(out3_2)\n",
    "        \n",
    "        # Residual connection\n",
    "        out3_2 += res_w * inp\n",
    "        \n",
    "        # Last activation\n",
    "        out3_2 = Activation(activation)(out3_2)\n",
    "        \n",
    "        return out3_2\n",
    "    \n",
    "    def reduction(inp, n_filters, initializer, activation='relu', bn=True):\n",
    "        # Split to three branches\n",
    "        # Branch 1\n",
    "        out1_1 = Conv2D(n_filters[0][0], (3,3), strides=(2,2), activation=activation, \n",
    "                        kernel_initializer=initializer, padding='same')(inp)\n",
    "        if bn:\n",
    "            out1_1 = BatchNormalization()(out1_1)\n",
    "        out1_2 = Conv2D(n_filters[0][1], (3,3), strides=(1,1), activation=activation, \n",
    "                        kernel_initializer=initializer, padding='same')(out1_1)\n",
    "        if bn:\n",
    "            out1_2 = BatchNormalization()(out1_2)\n",
    "        out1_3 = Conv2D(n_filters[0][2], (3,3), strides=(1,1), activation=activation, \n",
    "                        kernel_initializer=initializer, padding='same')(out1_2)\n",
    "        if bn:\n",
    "            out1_3 = BatchNormalization()(out1_3)\n",
    "        \n",
    "        # Branch 2\n",
    "        out2_1 = Conv2D(n_filters[1][0], (3,3), strides=(2,2), activation=activation, \n",
    "                        kernel_initializer=initializer, padding='same')(inp)\n",
    "        if bn:\n",
    "            out2_1 = BatchNormalization()(out2_1)\n",
    "        \n",
    "        # Branch 3\n",
    "        out3_1 = MaxPool2D((3,3), strides=(2,2), padding='same')(inp)\n",
    "        \n",
    "        # Concat the results from 3 branches\n",
    "        out = Concatenate(axis=-1)([out1_3, out2_1, out3_1])        \n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    activation=tf.nn.leaky_relu\n",
    "    init = tf.keras.initializers.GlorotUniform() #tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in')\n",
    "    \n",
    "    bn=True\n",
    "    \n",
    "    \n",
    "    inp = Input(shape=(64,64,3))\n",
    "    crop_inp = RandomCrop(56, 56, seed=random_seed)(inp)\n",
    "    crop_inp = RandomContrast(0.3, seed=random_seed)(crop_inp)\n",
    "    stem_out = stem(crop_inp)\n",
    "    \n",
    "    inc_a = inception_resnet_a(stem_out, [(32,),(32,32), (32, 48, 64, 384),(384,)], initializer=init)    \n",
    "\n",
    "    red = reduction(inc_a, [(256,256,384),(384,)], initializer=init)\n",
    "\n",
    "    inc_b1 = inception_resnet_b(red, [(192,),(128,160,192),(1152,)], initializer=init)\n",
    "    inc_b2 = inception_resnet_b(inc_b1,  [(192,),(128,160,192),(1152,)], initializer=init)    \n",
    "    \n",
    "    avgpool1 = AvgPool2D((4,4), strides=(1,1), padding='valid')(inc_b2)\n",
    "    flat_out = Flatten()(avgpool1)\n",
    "    dropout1 = Dropout(0.5)(flat_out)\n",
    "    out_main = Dense(200, activation='softmax',  kernel_initializer=init, name='final')(flat_out)\n",
    "\n",
    "    # Loss Weighing: https://github.com/tensorflow/models/blob/09d3c74a31d7e0c1742ae65025c249609b3c9d81/research/slim/train_image_classifier.py#L495\n",
    "    minception_resnet_v2 = Model(inputs=inp, outputs=out_main)\n",
    "    \n",
    "    minception_resnet_v2.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return minception_resnet_v2\n",
    "\n",
    "model = get_minception_resnet_v2()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "from functools import partial\n",
    "\n",
    "def get_steps_per_epoch(n_data, batch_size):\n",
    "    \"\"\" Given the data size and batch size, gives the number of steps to travers the full dataset \"\"\"\n",
    "    if n_data%batch_size==0:\n",
    "        return int(n_data/batch_size)\n",
    "    else:\n",
    "        return int(n_data*1.0/batch_size)+1\n",
    "    \n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "csv_logger = CSVLogger(os.path.join('eval','3_eval_minception.log'))\n",
    "n_epochs=5\n",
    "\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(    \n",
    "    monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='auto'\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "history = model.fit(\n",
    "    train_gen_aux, validation_data=valid_gen_aux, \n",
    "    steps_per_epoch=get_steps_per_epoch(int(0.9*(500*200)),batch_size), \n",
    "    validation_steps=get_steps_per_epoch(int(0.1*(500*200)),batch_size),\n",
    "    epochs=n_epochs , callbacks = [es_callback , csv_logger]\n",
    ")\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models'):\n",
    "    os.mkdir(\"models\")\n",
    "model.save(os.path.join('models', 'minception_resnet_v2.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk\n",
    "model = load_model(os.path.join('models','minception_resnet_v2.h5'))\n",
    "\n",
    "# Evaluate the model\n",
    "test_res = model.evaluate(test_gen_aux, steps=get_steps_per_epoch(500*50, batch_size))\n",
    "\n",
    "# Print the results as a dictionary {<metric name>: <value>}\n",
    "test_res_dict = dict(zip(model.metrics_names, test_res))\n",
    "print(test_res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
